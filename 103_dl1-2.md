[JDLA認定プログラム](http://study-ai.com/jdla)で、JDLA E資格に準拠した学習を行った。　　
　　
![Image](http://ai999.careers/bnr_jdla.png)

# 深層学習 Day1

## Section1 入力層～中間層

*万能近似定理：「深層ニューラルネットワークが、どんな関数でも近似できる」

*ノーフリーランチ定理：「あらゆる問題に対して性能の良い汎用最適化戦略は存在しない」

## Section2 活性化関数、Section3 出力層

*中間層用の活性化関数
    *ReLU関数：勾配消失問題を回避
    *シグモイド関数：両端で出力の変化が微笑で勾配消失問題が起こる
    *ステップ関数：線形分離可能なもののみ学習可
    
*出力層用の活性化関数
    *ソフトマックス関数
    *恒等写像
    *シグモイド関数
    *ソフトマックス関数

*誤差関数
    *平均二乗誤差（MSE:Mean Squared Error)：回帰問題
    *クロス（交差）エントロピー誤差：分類問題

## Section4 勾配降下法

*3種類
    *勾配降下法
    *確率的勾配降下法：ランダムに抽出したサンプル
        *オンライン学習が可
    *ミニバッチ勾配降下法：ランダムに分割したミニバッチ
        *オンライン学習が可
        *バッチを同時並行で学習可


## Section5 誤差逆伝播法

*誤差を出力側～入力側へ順に微分し、伝播する。計算量を減らすことができる。


# 深層学習 Day2

## Section1 勾配消失問題

*解決方法
    *活性化関数の選択
        *シグモイド関数ではなくReLU関数を使う。スパース化されるメリットがある。
        
    *重みの初期値設定
        *乱数を使って初期化する。
        *例１　Xavier（ザビエル）：S字カーブの活性化関数に使う
        *例２　He
        
    *バッチ正規化
        *ミニバッチ単位で入力値のデータの偏りを抑制する

## Section2 学習率最適化手法

*このページの解説が最もわかりやすい。　https://github.com/Jaewan-Yun/optimizer-visualization

    *モメンタム
        *局所的最適解でなく、大域的最適解となる
        *谷間から最適値へ早くたどりつく
    *AdaGrad
        *勾配の緩やかな場合に最適値へ近づきやすい
        *学習率が徐々に小さくなるので鞍点問題を起こす
    *RMSProp
        *AdaGradより局所的最適解でなく、大域的最適解となる
        *ハイパーパラメータ調整が必要になる場合が少ない
    *Adam
        *モメンタムとRMSPropの２つを含む
        *鞍点を抜けることができる

## Section3 過学習

*ネットワークの自由度（層数、ノード数、パラメータの値）が高すぎる、つまりNNが複雑すぎることが原因。

*正則化：NNの自由度を削ぐこと
    *一部の重みが大きすぎる（過大評価して、極端な反応を示す）
    *誤差に正則化項を加算することで重みを抑制する
    
        *ノルム：距離の事
        *p1ノルム：マンハッタン距離
            *直角三角形の2辺を足すイメージ
        *p2ノルム：ユークリッド距離
            *直角三角形の斜辺を求めるイメージ
    
    *L1正則化（ラッソ回帰）：p1ノルムを使った正則化
    
    *L2正則化（リッジ回帰）：p2ノルムを使った正則化

    *重みが2個の場合、L1正則化項はダイヤ型、L2正則化項はすり鉢型になる。


*ドロップアウト：ランダムにノードを削除する

## Section4 畳み込みNNの概念

*全結合層で画像を学習する場合、1次元のデータとして処理され、2次元（縦、横）、3次元（チャンネル）の関連性が学習に反映されない。その為に、畳み込み演算を使って学習する。

*プーリング層
    *畳み込み演算と異なり重みがない。
    *Max Pooling, Average Poolingなど
    
*AlexNetモデル
    *畳み込み層の後、全結合層を持つ。
    *全結合層にドロップアウトを使う。




