[JDLA認定プログラム](http://study-ai.com/jdla)で、JDLA E資格に準拠した学習を行った。　　
　　
![Image](http://ai999.careers/bnr_jdla.png)

# 深層学習 Day1

## Section1 入力層～中間層

*万能近似定理：「深層ニューラルネットワークが、どんな関数でも近似できる」

*ノーフリーランチ定理：「あらゆる問題に対して性能の良い汎用最適化戦略は存在しない」

## Section2 活性化関数、Section3 出力層

*中間層用の活性化関数
    *ReLU関数：勾配消失問題を回避
    *シグモイド関数：両端で出力の変化が微笑で勾配消失問題が起こる
    *ステップ関数：線形分離可能なもののみ学習可
    
*出力層用の活性化関数
    *ソフトマックス関数
    *恒等写像
    *シグモイド関数
    *ソフトマックス関数

*誤差関数
    *平均二乗誤差（MSE:Mean Squared Error)：回帰問題
    *クロス（交差）エントロピー誤差：分類問題

## Section4 勾配降下法

*3種類
    *勾配降下法
    *確率的勾配降下法：ランダムに抽出したサンプル
        *オンライン学習が可
    *ミニバッチ勾配降下法：ランダムに分割したミニバッチ
        *オンライン学習が可
        *バッチを同時並行で学習可


## Section5 誤差逆伝播法

*誤差を出力側～入力側へ順に微分し、伝播する。計算量を減らすことができる。


# 深層学習 Day2

## Section1 勾配消失問題

*解決方法
    *活性化関数の選択
        *シグモイド関数ではなくReLU関数を使う。スパース化されるメリットがある。
        
    *重みの初期値設定
        *乱数を使って初期化する。
        *例１　Xavier（ザビエル）：S字カーブの活性化関数に使う
        *例２　He
        
    *バッチ正規化
        *ミニバッチ単位で入力値のデータの偏りを抑制する

## Section2 学習率最適化手法

*このページの解説が最もわかりやすい。　https://github.com/Jaewan-Yun/optimizer-visualization

    *モメンタム
        *局所的最適解でなく、大域的最適解となる
        *谷間から最適値へ早くたどりつく
    *AdaGrad
        *勾配の緩やかな場合に最適値へ近づきやすい
        *学習率が徐々に小さくなるので鞍点問題を起こす
    *RMSProp
        *AdaGradより局所的最適解でなく、大域的最適解となる
        *ハイパーパラメータ調整が必要になる場合が少ない
    *Adam
        *モメンタムとRMSPropの２つを含む
        *鞍点を抜けることができる

## Section3 過学習

*ネットワークの自由度（層数、ノード数、パラメータの値）が高すぎる、つまりNNが複雑すぎることが原因。

*正則化：NNの自由度を削ぐこと
    *一部の重みが大きすぎる（過大評価して、極端な反応を示す）
    *誤差に正則化項を加算することで重みを抑制する
    
        *ノルム：距離の事
        *p1ノルム：マンハッタン距離
            *直角三角形の2辺を足すイメージ
        *p2ノルム：ユークリッド距離
            *直角三角形の斜辺を求めるイメージ
    
    *L1正則化（ラッソ回帰）：p1ノルムを使った正則化
    
    *L2正則化（リッジ回帰）：p2ノルムを使った正則化

    *重みが2個の場合、L1正則化項はダイヤ型、L2正則化項はすり鉢型になる。


*ドロップアウト：ランダムにノードを削除する

## Section4 畳み込みNNの概念

*全結合層で画像を学習する場合、1次元のデータとして処理され、2次元（縦、横）、3次元（チャンネル）の関連性が学習に反映されない。その為に、畳み込み演算を使って学習する。

*プーリング層
    *畳み込み演算と異なり重みがない。
    *Max Pooling, Average Poolingなど
    
*AlexNetモデル
    *畳み込み層の後、全結合層を持つ。
    *全結合層にドロップアウトを使う。

# 深層学習 Day3

## 再帰型ニューラルネットワーク
### RNN

* Recurrent Neural Network（RNN)
    *時系列データに対応可能
    *初期の状態と過去の時間 t−1の状態を保持し、次の時間 t の状態を求める再帰的構造

    *課題：時系列をさかのぼるほど勾配が消失し長い時系列の学習が困難→LSTMが解決
    
    *課題：勾配爆発→勾配のクリッピング

### BPTT

* RNNにおけるパラメータ調整方法、誤差逆伝播の一種

* 誤差逆伝播の復習：誤差から微分を逆算し再帰的な計算を減らして微分を算出する

## LSTM

* CEC (Constant Error Carousel)
    *入力ゲート、出力ゲートを追加し解決

* 忘却ゲート：不要になった過去の情報を忘却する

* 覗き穴結合

* 課題：パラメータ数が多く、計算負荷が高い。パラメータを削減し、精度は同等／それ以上を望むのがGRU（Gated Recurrent Unit）。

## 双方向RNN

* 過去の情報だけでなくう未来も加味し精度を向上させる

## RNNでの自然言語処理

### Seq2Seq

* Encoder Decoder モデルの１種で、機械対話・機械翻訳に使用される。

* Encoder RNN：テキストを単語等のトークンに区切って渡す

* Decoder RNN：アウトプットを単語等のトークンごとに生成する

* HRED：過去の発話から次の発話を生成する。文脈に即した応答の為、表面上「人間らしい」と感じられる。

*VHRED：HREDにVAEの潜在変数の概念を追加した。

*VAE
    * オートエンコーダ：教師なし学習。入力データを潜在変数に変換するEncoderと、潜在変数から元と同じデータを復元するDecoder。
    * VAE：潜在変数に確率分布を仮定

### Word2vec

* ボキャブラリを生成し、ボキャブラリ数×任意の単語ベクトルの次元数の行列により学習する。（計算量を削減）


### AttentionMechanism

* 入力のどの単語と、出力のどの単語が関連しているかを学習。


# 深層学習 Day4

# 強化学習

* 長期的に報酬を最大化できるように環境の中で行動を選択するエージェントを作る機械学習
    * （例）DMを送るエージェントの場合、コスト（負の報酬）と売上（正の報酬）を最大化する行動（顧客ごとの送信するか否か）を選択する。
    * 探索と利用のトレードオフ：探索しないと、よりよい行動を見つけることができない。探索ばかりすると経験を生かせない。
    * 教師あり／なし学習が「パターンを見つけ予測する」に対し、強化学習は「優れた方策を見つける」のが目標
     
     * Q学習、関数近似法
     
     * 状態価値関数、行動価値関数
     
     * 方策関数
     
     * 方策勾配法
     

# 軽量化、高速化

* 分散深層学習：並列的にNNNを構成し、複数の計算資源を並行して使う。
    * データ並列化（同期型・非同期型）
    * モデル並列化
    
* GPUによる高速化
    * CUDA, OpenCL

* モデルの軽量化　https://www.oki.com/jp/otr/2019/n233/pdf/otr233_r11.pdf
    
    * 量子化（Quantization）：極端に精度が落ちないよう有効桁数を落とす
    
    * 蒸留（Distillation）：軽量なモデルに知識を継承させる
        * 教師モデル（重みを固定）との誤差が小さくなるよう生徒モデルの重みを更新する。
    
    * プルーニング（Pruning）：モデル精度に寄与しないニューロンを減らす
    

# 応用技術

* MobileNets　https://qiita.com/HiromuMasuda0228/items/7dd0b764804d2aa199e4
    * 畳み込み計算量を減らす。
    * Depthwise ConvolutionとPointwise Convolutionの組み合わせ

* DenseNet　https://www.slideshare.net/harmonylab/densely-connected-convolutional-networks

* Batch Norm Layer　https://arxiv.org/pdf/1803.08494.pdf

* Layer Norm　https://www.slideshare.net/KeigoNishida/layer-normalizationnips

* Instance Norm

* Wavenet


